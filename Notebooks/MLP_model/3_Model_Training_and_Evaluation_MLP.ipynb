{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "3 Model Training and Evaluation_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxeMiXk92l8W"
      },
      "source": [
        "# Classifying CLAP(spam +non spam) using MLP architecture\n",
        "\n",
        "## 3 Model Training and Evaluation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTYKIld62l8h"
      },
      "source": [
        "### Load Preprocessed data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9yphETQitsk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wpSiNd422n4",
        "outputId": "10634718-4499-4c8d-b742-cf0b2dfac20d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoMwPJut27_1"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os,sys\n",
        "\n",
        "import seaborn as sns\n",
        "import wave\n",
        "import librosa\n",
        "import numpy as np\n",
        "import scipy\n",
        "from tqdm import tqdm_notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd  # to play sound in notebook\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.utils import Sequence, to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras import losses, models, optimizers\n",
        "from keras.activations import relu, softmax\n",
        "from keras.layers import Dense, Dropout, Input, Convolution2D, BatchNormalization, Activation, MaxPool2D, Flatten\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-A_SBkS3E3O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "1b593ca9-0bdd-4581-f3a8-d65e53eb8d74"
      },
      "source": [
        "ROOT_PATH='./drive/MyDrive/ASR_Project_Shared/'\n",
        "\n",
        "## Creating a softlink to drive root, easy for relative addressing\n",
        "\n",
        "## Guys \n",
        "\n",
        "\n",
        "relative_path = 'final_metadata/denoised/'\n",
        "\n",
        "\n",
        "\n",
        "nb_path = './normal'\n",
        "os.symlink(ROOT_PATH+'final_metadata/normal', nb_path)\n",
        "sys.path.insert(0, nb_path) \n",
        "\n",
        "\n",
        "nb_path = './denoised'\n",
        "os.symlink(ROOT_PATH+'final_metadata/denoised', nb_path)\n",
        "sys.path.insert(0, nb_path) \n",
        "\n",
        "\n",
        "\n",
        "nb_path = './final_data'\n",
        "os.symlink(ROOT_PATH+'final_data', nb_path)\n",
        "sys.path.insert(0, nb_path) \n",
        "\n",
        "\n",
        "metadata_file = 'train_metadata_speech{all_clap}_noise{all_clap+noiseclips}.csv'\n",
        "\n",
        "pickle_file_name = metadata_file +'.pkl'\n",
        "nb_path = './'+ metadata_file\n",
        "os.symlink(ROOT_PATH+relative_path+metadata_file, nb_path)\n",
        "sys.path.insert(0, nb_path) \n",
        "\n",
        "nb_path = './'+ pickle_file_name\n",
        "os.symlink(ROOT_PATH+relative_path+pickle_file_name, nb_path)\n",
        "sys.path.insert(0, nb_path) \n",
        "\n",
        "metadata_file = 'test_metadata_speech{all_clap}_noise{qut+spam} - test_metadata_speech{all_clap}_noise{qut+spam}.csv'\n",
        "\n",
        "pickle_file_name = metadata_file +'.pkl'\n",
        "nb_path = './'+ metadata_file\n",
        "os.symlink(ROOT_PATH+relative_path+metadata_file, nb_path)\n",
        "sys.path.insert(0, nb_path) \n",
        "\n",
        "nb_path = './'+ pickle_file_name\n",
        "os.symlink(ROOT_PATH+relative_path+pickle_file_name, nb_path)\n",
        "sys.path.insert(0, nb_path) \n",
        "\n",
        "metadata_file = 'test_metadata_speech{all_clap}_noise{all_clap+noiseclips+spam} - test_metadata_speech{all_clap}_noise{all_clap+noiseclips+spam}.csv'\n",
        "\n",
        "pickle_file_name = metadata_file +'.pkl'\n",
        "nb_path = './'+ metadata_file\n",
        "os.symlink(ROOT_PATH+relative_path+metadata_file, nb_path)\n",
        "sys.path.insert(0, nb_path) \n",
        "\n",
        "nb_path = './'+ pickle_file_name\n",
        "os.symlink(ROOT_PATH+relative_path+pickle_file_name, nb_path)\n",
        "sys.path.insert(0, nb_path) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2cd48c5b7c50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnb_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./normal'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'final_metadata/normal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './drive/MyDrive/ASR_Project_Shared/final_metadata/normal' -> './normal'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwP0d1IEscd5",
        "outputId": "a81951a3-4906-455e-a125-9cb041cdf104"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUq_MaQI2l8i"
      },
      "source": [
        "# # retrieve the preprocessed data from previous notebook\n",
        "\n",
        "# %store -r x_train \n",
        "# %store -r x_test \n",
        "# %store -r y_train \n",
        "# %store -r y_test \n",
        "# %store -r yy \n",
        "# %store -r le\n",
        "\n",
        "import pickle\n",
        "featuresdf =  pickle.load( open( \"train_metadata_speech{all_clap}_noise{all_clap+noiseclips}.csv.pkl\", \"rb\" ) )\n",
        "#featuresdf = featuresdf.sample(frac=1).reset_index(drop=True)\n",
        "features_marathidf = pickle.load( open( \"test_metadata_speech{all_clap}_noise{qut+spam} - test_metadata_speech{all_clap}_noise{qut+spam}.csv.pkl\", \"rb\" ) )\n",
        "#features_marathidf = features_marathidf.sample(frac=1).reset_index(drop=True)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Convert features and corresponding classification labels into numpy arrays\n",
        "X = np.array(featuresdf.feature.tolist())\n",
        "y = np.array(featuresdf.class_label.tolist())\n",
        "\n",
        "# Encode the classification labels\n",
        "le = LabelEncoder()\n",
        "yy = to_categorical(le.fit_transform(y)) \n",
        "# split the dataset \n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "#x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n",
        "\n",
        "x_train = X\n",
        "y_train = yy\n",
        "\n",
        "\n",
        "X = np.array(features_marathidf.feature.tolist())\n",
        "y = np.array(features_marathidf.class_label.tolist())\n",
        "\n",
        "# Encode the classification labels\n",
        "le = LabelEncoder()\n",
        "yy = to_categorical(le.fit_transform(y)) \n",
        "\n",
        "x_test = X\n",
        "y_test = yy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIXFHurWTr_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660028b7-399b-435b-df53-b2228dbf724c"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       ...,\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfeMoZ4f2l8i"
      },
      "source": [
        "### Initial model architecture - MLP\n",
        "\n",
        "We will start with constructing a Multilayer Perceptron (MLP) Neural Network using Keras and a Tensorflow backend. \n",
        "\n",
        "Starting with a `sequential` model so we can build the model layer by layer. \n",
        "\n",
        "We will begin with a simple model architecture, consisting of three layers, an input layer, a hidden layer and an output layer. All three layers will be of the `dense` layer type which is a standard layer type that is used in many cases for neural networks. \n",
        "\n",
        "The first layer will receive the input shape. As each sample contains 40 MFCCs (or columns) we have a shape of (1x40) this means we will start with an input shape of 40. \n",
        "\n",
        "The first two layers will have 256 nodes. The activation function we will be using for our first 2 layers is the `ReLU`, or `Rectified Linear Activation`. This activation function has been proven to work well in neural networks.\n",
        "\n",
        "We will also apply a `Dropout` value of 50% on our first two layers. This will randomly exclude nodes from each update cycle which in turn results in a network that is capable of better generalisation and is less likely to overfit the training data.\n",
        "\n",
        "Our output layer will have 10 nodes (num_labels) which matches the number of possible classifications. The activation is for our output layer is `softmax`. Softmax makes the output sum up to 1 so the output can be interpreted as probabilities. The model will then make its prediction based on which option has the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPw9sQYd2l8j"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D , Conv2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from sklearn import metrics \n",
        "\n",
        "num_labels = yy.shape[1]\n",
        "filter_size = 2\n",
        "\n",
        "# Construct model \n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(256, input_shape=(40,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-JqRhqdqtgv"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D , Conv2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from sklearn import metrics "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsDSTba2l8l"
      },
      "source": [
        "### Compiling the model \n",
        "\n",
        "For compiling our model, we will use the following three parameters: \n",
        "\n",
        "* Loss function - we will use `categorical_crossentropy`. This is the most common choice for classification. A lower score indicates that the model is performing better.\n",
        "\n",
        "* Metrics - we will use the `accuracy` metric which will allow us to view the accuracy score on the validation data when we train the model. \n",
        "\n",
        "* Optimizer - here we will use `adam` which is a generally good optimizer for many use cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq4sM_sx2l8l"
      },
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6wKPI2U2l8m",
        "outputId": "2501241e-6d9d-442d-9bfe-f78b1db45fd9"
      },
      "source": [
        "# Display model architecture summary \n",
        "model.summary()\n",
        "\n",
        "# Calculate pre-training accuracy \n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "accuracy = 100*score[1]\n",
        "\n",
        "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 256)               10496     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 2)                 514       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 76,802\n",
            "Trainable params: 76,802\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Pre-training accuracy: 35.9649%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p5vRlxN2l8m"
      },
      "source": [
        "### Training \n",
        "\n",
        "Here we will train the model. \n",
        "\n",
        "We will start with 100 epochs which is the number of times the model will cycle through the data. The model will improve on each cycle until it reaches a certain point. \n",
        "\n",
        "We will also start with a low batch size, as having a large batch size can reduce the generalisation ability of the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKakbbXF2l8n",
        "outputId": "0b84e3d5-a1a7-4bde-860f-49979baeaad3"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint \n",
        "from datetime import datetime \n",
        "\n",
        "num_epochs = 10\n",
        "num_batch_size = 32\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "start = datetime.now()\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 8.6341 - accuracy: 0.8255 - val_loss: 1.8351 - val_accuracy: 0.9259\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.83513, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "Epoch 2/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 2.0492 - accuracy: 0.9026 - val_loss: 0.5464 - val_accuracy: 0.9220\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.83513 to 0.54642, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "Epoch 3/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 0.9655 - accuracy: 0.9039 - val_loss: 0.2960 - val_accuracy: 0.9181\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.54642 to 0.29595, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "Epoch 4/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 0.5392 - accuracy: 0.9051 - val_loss: 0.3398 - val_accuracy: 0.9230\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.29595\n",
            "Epoch 5/10\n",
            "141/141 [==============================] - 0s 4ms/step - loss: 0.3135 - accuracy: 0.9131 - val_loss: 0.3090 - val_accuracy: 0.9230\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.29595\n",
            "Epoch 6/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 0.2818 - accuracy: 0.9209 - val_loss: 0.3364 - val_accuracy: 0.8996\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.29595\n",
            "Epoch 7/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 0.2336 - accuracy: 0.9276 - val_loss: 0.2674 - val_accuracy: 0.9288\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.29595 to 0.26745, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "Epoch 8/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 0.2235 - accuracy: 0.9327 - val_loss: 0.2900 - val_accuracy: 0.9230\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.26745\n",
            "Epoch 9/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 0.2015 - accuracy: 0.9360 - val_loss: 0.3227 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.26745\n",
            "Epoch 10/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 0.2013 - accuracy: 0.9389 - val_loss: 0.2611 - val_accuracy: 0.9288\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.26745 to 0.26107, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "Training completed in time:  0:00:05.429540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lReATZ52l8n"
      },
      "source": [
        "### Test the model \n",
        "\n",
        "Here we will review the accuracy of the model on both the training and test data sets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn1g4GK22l8o",
        "outputId": "1548cf62-6438-42ae-9dbc-6c214a333f28"
      },
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = model.evaluate(x_train, y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.9587697982788086\n",
            "Testing Accuracy:  0.9288498759269714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSuhWWRFKFYC"
      },
      "source": [
        "y_pred = (model.predict(x_test) > 0.5).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0DIf94sQOU3",
        "outputId": "dd3af30d-c45f-4838-b85b-34d248173493"
      },
      "source": [
        "y_pred_ = model.predict(x_test, use_multiprocessing=True, workers=6, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 0s 1ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQHog4XtRDmX"
      },
      "source": [
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, labels):\n",
        "    \"\"\"\n",
        "    plots the confusion matrix\n",
        "    \"\"\"\n",
        "    matrix = confusion_matrix(y_true, y_pred)    \n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    plt.imshow(matrix)\n",
        "    ax.set_xticks(range(len(labels)));\n",
        "    ax.set_xticklabels(labels, rotation=0)\n",
        "    ax.set_yticks(range(len(labels)));\n",
        "    ax.set_yticklabels(labels)\n",
        "    max_confusions = 0\n",
        "    confused_classes = (-1, -1)\n",
        "    for i, true_label in enumerate(matrix):\n",
        "        for j, predicted_label in enumerate(true_label):\n",
        "            text = ax.text(j, i, matrix[i, j],\n",
        "                        ha=\"center\", va=\"center\", color=\"r\");\n",
        "    plt.tick_params(axis=u'both', which=u'both',length=0)\n",
        "    plt.title(\"Confusion Matrix\");\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "Qbc2icXIHRz9",
        "outputId": "f5f44c48-b3c2-48b2-f847-9cf3ce0c2dcc"
      },
      "source": [
        "\n",
        "y_pred = np.argmax(y_pred_, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "#labels = []_\n",
        "labels =['spam','not spam']\n",
        "\n",
        "plot_confusion_matrix(y_true, y_pred, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI5nDAs-2l8o"
      },
      "source": [
        "The initial Training and Testing accuracy scores are quite high. As there is not a great difference between the Training and Test scores (~5%) this suggests that the model has not suffered from overfitting. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spBAT5Pp2l8p"
      },
      "source": [
        "### Predictions  \n",
        "\n",
        "Here we will build a method which will allow us to test the models predictions on a specified audio .wav file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD63Odo82l8p"
      },
      "source": [
        "import librosa \n",
        "import numpy as np \n",
        "\n",
        "def extract_feature(file_name):\n",
        "   \n",
        "    try:\n",
        "        audio_data, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
        "        mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40)\n",
        "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(\"Error encountered while parsing file: \", file)\n",
        "        return None, None\n",
        "\n",
        "    return np.array([mfccsscaled])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWDyRgAv2l8q"
      },
      "source": [
        "def print_prediction(file_name):\n",
        "    prediction_feature = extract_feature(file_name) \n",
        "\n",
        "    predicted_vector = model.predict_classes(prediction_feature)\n",
        "    predicted_class = le.inverse_transform(predicted_vector) \n",
        "    print(\"The predicted class is:\", predicted_class[0], '\\n') \n",
        "\n",
        "    predicted_proba_vector = model.predict_proba(prediction_feature) \n",
        "    predicted_proba = predicted_proba_vector[0]\n",
        "    for i in range(len(predicted_proba)): \n",
        "        category = le.inverse_transform(np.array([i]))\n",
        "        print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f') )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISOeVR-B2l8q"
      },
      "source": [
        "### Validation \n",
        "\n",
        "#### Test with sample data \n",
        "\n",
        "Initial sainity check to verify the predictions using a subsection of the sample audio files we explored in the first notebook. We expect the bulk of these to be classified correctly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsDKHUfK2l8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18428db5-3eb4-4bc6-8579-66e64b98967d"
      },
      "source": [
        "# Class: Air Conditioner\n",
        "\n",
        "filename = 'final_data/noise/CAFE-CAFE-1_trim_5s_505.wav' \n",
        "print_prediction(filename) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The predicted class is: 1 \n",
            "\n",
            "0 \t\t :  0.00000000002346651345463968141303\n",
            "1 \t\t :  1.00000000000000000000000000000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90UFrIF126gA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
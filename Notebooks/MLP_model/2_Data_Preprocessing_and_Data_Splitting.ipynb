{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 Data Preprocessing and Data Splitting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p59Y4pNfeJmK"
      },
      "source": [
        "# Classifying CLAP(spam +non spam) using data splitting\n",
        "\n",
        "## 2 Data Preprocessing and Data Splitting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSdFmLe8eJmP"
      },
      "source": [
        "### Audio properties that will require normalising \n",
        "\n",
        "Following on from the previous notebook, we identifed the following audio properties that need preprocessing to ensure consistency across the whole dataset:  \n",
        "\n",
        "- Audio Channels \n",
        "- Sample rate \n",
        "- Bit-depth\n",
        "\n",
        "We will continue to use Librosa which will be useful for the pre-processing and feature extraction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7xc9UDreJmP"
      },
      "source": [
        "### Preprocessing stage \n",
        "\n",
        "For much of the preprocessing we will be able to use [Librosa's load() function.](https://librosa.github.io/librosa/generated/librosa.core.load.html) \n",
        "\n",
        "We will compare the outputs from Librosa against the default outputs of [scipy's wavfile library](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.io.wavfile.read.html) using a chosen file from the dataset. \n",
        "\n",
        "#### Sample rate conversion \n",
        "\n",
        "By default, Librosa’s load function converts the sampling rate to 22.05 KHz which we can use as our comparison level. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIACMVPVeQKn",
        "outputId": "ded3794f-304c-4798-ec46-b748dadc1429"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB4kpwOveWmi"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os,sys\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa2ZmT9oeruy"
      },
      "source": [
        "ROOT_PATH='./drive/MyDrive/ASR_Project_Shared/'\n",
        "\n",
        "## Creating a softlink to drive root, easy for relative addressing\n",
        "\n",
        "## Guys \n",
        "relative_path = 'final_metadata/denoised/'\n",
        "\n",
        "metadata_file = 'test_metadata_speech{all_clap}_noise{qut+spam} - test_metadata_speech{all_clap}_noise{qut+spam}.csv'\n",
        "\n",
        "\n",
        "# nb_path = './normal'\n",
        "# os.symlink(ROOT_PATH+'final_metadata/normal', nb_path)\n",
        "# sys.path.insert(0, nb_path) \n",
        "\n",
        "\n",
        "# nb_path = './denoised'\n",
        "# os.symlink(ROOT_PATH+'final_metadata/denoised', nb_path)\n",
        "# sys.path.insert(0, nb_path) \n",
        "\n",
        "pickle_file_name = relative_path.split('/')[-2]+metadata_file\n",
        "nb_path = './'+ metadata_file\n",
        "os.symlink(ROOT_PATH+relative_path+metadata_file, nb_path)\n",
        "sys.path.insert(0, nb_path) \n",
        "\n",
        "nb_path = './final_data'\n",
        "os.symlink(ROOT_PATH+'final_data', nb_path)\n",
        "sys.path.insert(0, nb_path) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RVyKMP_PP4cn",
        "outputId": "d028f58c-cdbe-43a7-c386-cfe5e58ccbda"
      },
      "source": [
        "pickle_file_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'normalTrain_metadata_speech{all_clap}_noise{all_clap+noiseclips}.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRmjHLekeJmQ"
      },
      "source": [
        "# import librosa \n",
        "# from scipy.io import wavfile as wav\n",
        "# import numpy as np\n",
        "\n",
        "# filename = 'final_data/noise/CAFE-CAFE-1_trim_5s_505.wav' \n",
        "\n",
        "# librosa_audio, librosa_sample_rate = librosa.load(filename) \n",
        "# scipy_sample_rate, scipy_audio = wav.read(filename) \n",
        "\n",
        "# print('Original sample rate:', scipy_sample_rate) \n",
        "# print('Librosa sample rate:', librosa_sample_rate) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PVLIl7keJmR"
      },
      "source": [
        "#### Bit-depth \n",
        "\n",
        "Librosa’s load function will also normalise the data so it's values range between -1 and 1. This removes the complication of the dataset having a wide range of bit-depths. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrUK0T8AeJmR"
      },
      "source": [
        "# print('Original audio file min~max range:', np.min(scipy_audio), 'to', np.max(scipy_audio))\n",
        "# print('Librosa audio file min~max range:', np.min(librosa_audio), 'to', np.max(librosa_audio))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5AfzxM0eJmR"
      },
      "source": [
        "#### Merge audio channels \n",
        "\n",
        "Librosa will also convert the signal to mono, meaning the number of channels will always be 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzZyXzWEeJmS"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Original audio with 2 channels \n",
        "# plt.figure(figsize=(12, 4))\n",
        "# plt.plot(scipy_audio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3PmCbmMeJmS"
      },
      "source": [
        "# # Librosa audio with channels merged \n",
        "# plt.figure(figsize=(12, 4))\n",
        "# plt.plot(librosa_audio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkdfwgaueJmS"
      },
      "source": [
        "#### Other audio properties to consider\n",
        "\n",
        "At this stage it is not yet clear whether other factors may also need to be taken into account, such as sample duration length and volume levels. \n",
        "\n",
        "We will proceed as is for the meantime and come back to address these later if it's perceived to be effecting the validity of our target metrics. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6dCcgNUeJmT"
      },
      "source": [
        "### Extract Features \n",
        "\n",
        "As outlined in the proposal, we will extract [Mel-Frequency Cepstral Coefficients (MFCC)](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) from the the audio samples. \n",
        "\n",
        "The MFCC summarises the frequency distribution across the window size, so it is possible to analyse both the frequency and time characteristics of the sound. These audio representations will allow us to identify features for classification. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItW866FgeJmT"
      },
      "source": [
        "#### Extracting a MFCC\n",
        "\n",
        "For this we will use [Librosa's mfcc() function](https://librosa.github.io/librosa/generated/librosa.feature.mfcc.html) which generates an MFCC from time series audio data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyITL1w6eJmT"
      },
      "source": [
        "# mfccs = librosa.feature.mfcc(y=librosa_audio, sr=librosa_sample_rate, n_mfcc=40)\n",
        "# print(mfccs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf8OkKO1eJmT"
      },
      "source": [
        "This shows librosa calculated a series of 40 MFCCs over 173 frames. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX1mUqsAeJmT"
      },
      "source": [
        "# import librosa.display\n",
        "# librosa.display.specshow(mfccs, sr=librosa_sample_rate, x_axis='time')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "298it6Z3eJmU"
      },
      "source": [
        "#### Extracting MFCC's for every file \n",
        "\n",
        "We will now extract an MFCC for each audio file in the dataset and store it in a Panda Dataframe along with it's classification label. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-YRQODveJmU"
      },
      "source": [
        "def extract_features(file_name):\n",
        "   \n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(\"Error encountered while parsing file: \", file)\n",
        "        return None \n",
        "     \n",
        "    return mfccsscaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNpH7IoneJmU",
        "outputId": "445f2cf2-3d74-4a25-df86-6cc4c153ae2b"
      },
      "source": [
        "# Load various imports \n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "\n",
        "# Set the path to the full UrbanSound dataset \n",
        "#fulldatasetpath = '/Volumes/Untitled/ML_Data/Urban Sound/UrbanSound8K/audio/'\n",
        "\n",
        "metadata = pd.read_csv(metadata_file)\n",
        "\n",
        "features = []\n",
        "\n",
        "# Iterate through each sound file and extract the features \n",
        "for index, row in metadata.iterrows():\n",
        "    \n",
        "    file_name = row['file_path']\n",
        "    \n",
        "    class_label = row[\"label\"]\n",
        "    data = extract_features(file_name)\n",
        "    \n",
        "    features.append([data, class_label])\n",
        "\n",
        "# Convert into a Panda dataframe \n",
        "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
        "\n",
        "print('Finished feature extraction from ', len(featuresdf), ' files') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished feature extraction from  1026  files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OAz8U8QuTe-"
      },
      "source": [
        "#featuresdf = []\n",
        "import pickle\n",
        "path = ROOT_PATH\n",
        "pickle_out = open(path+ relative_path+ metadata_file+\".pkl\",\"wb\")  \n",
        "pickle.dump( featuresdf, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drdhulvkuSw9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6vgxJTReJmU"
      },
      "source": [
        "### Convert the data and labels\n",
        "\n",
        "We will use `sklearn.preprocessing.LabelEncoder` to encode the categorical text data into model-understandable numerical data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHvmipCueJmV"
      },
      "source": [
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from keras.utils import to_categorical\n",
        "\n",
        "# # Convert features and corresponding classification labels into numpy arrays\n",
        "# X = np.array(featuresdf.feature.tolist())\n",
        "# y = np.array(featuresdf.class_label.tolist())\n",
        "\n",
        "# # Encode the classification labels\n",
        "# le = LabelEncoder()\n",
        "# yy = to_categorical(le.fit_transform(y)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdZI-NaZeJmV"
      },
      "source": [
        "### Split the dataset\n",
        "\n",
        "Here we will use `sklearn.model_selection.train_test_split` to split the dataset into training and testing sets. The testing set size will be 20% and we will set a random state. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99OtJjGYeJmV"
      },
      "source": [
        "# # split the dataset \n",
        "# from sklearn.model_selection import train_test_split \n",
        "\n",
        "# x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KetbGSgKeJmV"
      },
      "source": [
        "### Store the preprocessed data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1Rp4m5WeJmW"
      },
      "source": [
        "### store the preprocessed data for use in the next notebook\n",
        "\n",
        "# %store x_train \n",
        "# %store x_test \n",
        "# %store y_train \n",
        "# %store y_test \n",
        "# %store yy \n",
        "# %store le"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpFpwyp0eJmW"
      },
      "source": [
        "### *In the next notebook we will develop our model*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tH9UnGseVSD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}